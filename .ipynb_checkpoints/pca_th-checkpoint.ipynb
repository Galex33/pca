{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=36><center><b>PCA (Primary Component Analysis)</b></center></font>\n",
    "(ACP : Analyse en Composantes Principales)  \n",
    "\n",
    "\n",
    "<img src=\"img/clustering.png\" width=\"400\"/>\n",
    "\n",
    "# Introduction\n",
    "L'analyse en composantes principales (ACP) est essentielle pour la science des données, l'apprentissage automatique, la visualisation des données, les statistiques et d'autres domaines quantitatifs.\n",
    "\n",
    "L’ACP est cruciale et stratégique pour un data analyst. Il est donc très important de bien la comprendre. Elle nécessite un minimum de pratique pour analyser les données correctement. **Si la compréhension est approximative, elle mène alors très facilement à des analyses erronées, approximatives.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexte:\n",
    "Lors de la mise en œuvre d'algorithmes d'apprentissage automatique, l'inclusion d'un plus grand nombre de features peut entraîner une aggravation des problèmes de performance. L'augmentation du nombre de features n'améliore pas toujours la précision de la classification, ce qui est également connu comme :  \n",
    "\n",
    "    le fléau de la dimensionnalité.\n",
    "<img src=\"img/dimensionality_vs_performance.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problématique :\n",
    "Lorsqu’on étudie simultanément un nombre important de variables quantitatives (ne serait-ce que 4 !), comment en faire un graphique global ? La difficulté vient de ce que les individus étudiés ne sont plus représentés dans un plan, espace de dimension 2, mais dans un espace de dimension plus importante (par exemple  4).  L’objectif  de  l’Analyse  en  Composantes  Principales  (ACP)  est de revenir à un espace de dimension réduite (par exemple 2) en déformant le moins possible la réalité. \n",
    "\n",
    "    Il s’agit donc d’obtenir le résumé le plus pertinent possible des données initiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principe :\n",
    "\n",
    "Le principe de l'ACP est d'appliquer une :\n",
    "    \n",
    "    la réduction de la dimensionnalité\n",
    "pour améliorer la précision de la classification en sélectionnant l'ensemble optimal de caractéristiques de plus faible dimensionnalité.\n",
    "\n",
    "<img src=\"img/projection.png\" width=\"300\"/>\n",
    "\n",
    "Une chose importante à noter à propos de l'ACP est qu'il s'agit d'une technique de réduction de dimensionnalité **non supervisée**, vous pouvez regrouper les points de données similaires en fonction de la corrélation de caractéristiques entre eux sans aucune supervision (ou étiquettes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctionnement :\n",
    "\n",
    "C’est la **matrice des variances-covariances** (ou celle des corrélations) qui va permettre de réaliser ce résumé pertinent, parce qu’on analyse essentiellement : \n",
    "\n",
    "    la dispersion des données considérées.\n",
    "    \n",
    "De cette matrice, on va extraire, par un  procédé  mathématique  adéquat,  les  facteurs  que  l’on  recherche,  en  petit nombre. Ils vont permettre de réaliser les graphiques désirés dans cet espace de petite dimension (le nombre de facteurs retenus), en déformant le moins possible la configuration globale des individus selon l’ensemble des variables initiales (ainsi remplacées par les facteurs).\n",
    "\n",
    "*D’un point de vue plus “mathématique”, l’ACP correspond à l’approximation d’une matrice (n,p) par une matrice de même dimensions mais de rang q < p ; q étant souvent de petite valeur (2, 3) pour la construction de graphiques facilement compréhensibles.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points de vigilance :\n",
    "\n",
    "## Les données doivent être standardisées.\n",
    "\n",
    "L'objectif est de pouvoir comparer les variables. Généralement les variables sont mises à l'échelle pour avoir :\n",
    "- un écart-type de un\n",
    "- une moyenne de zéro.\n",
    "\n",
    "## L'analyse en composantes principales est conçue pour les variables continues.\n",
    "\n",
    "Bien que l'on puisse utiliser l'ACP sur des données binaires (par exemple en encodant avec one-hot), cela ne signifie pas que ce soit une bonne manière de faire, ou que cela donnera des résultats pertinents.\n",
    "  \n",
    "    L'ACP essaye de minimiser la variance (= écarts quadratiques). Le concept d'écart au carré s'effondre lorsque vous avez des variables binaires.\n",
    "    \n",
    "Donc oui, vous pouvez utiliser l'ACP. Et oui, vous obtenez un résultat. Il s'agit même d'un résultat des moindres carrés - ce n'est pas comme si l'ACP était défaillante sur de telles données. Cela fonctionne, mais c'est beaucoup moins significatif que vous ne le souhaiteriez, et supposément moins significatif que, par exemple, l'extraction de motifs fréquents.\n",
    "\n",
    "## Représentation de la variance\n",
    "\n",
    "Nous devons prendre les vecteurs propres qui représentent le mieux notre dataset. Ce sont les vecteurs qui ont les valeurs propres les plus élevées.\n",
    "\n",
    "Selon Machine Learnia il est habituel de prendre les vecteurs capturant au moins 90 à 95% de la variance.\n",
    "\n",
    "<img src=\"img/variance.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprendre l'ACP\n",
    "\n",
    "- Aperçu du processus :\n",
    "    1. La technique de l'ACP analyse l'ensemble des données et trouve ensuite les points dont la variance est maximale.\n",
    "    2. Elle crée de nouvelles variables de sorte qu'il existe une relation linéaire entre les nouvelles variables et les variables d'origine, de sorte que la variance soit maximisée.\n",
    "    3. Une matrice de covariance est ensuite créée pour les caractéristiques afin de comprendre leur multi-collinéarité.\n",
    "    4. Une fois la matrice de variance-covariance calculée, l'ACP utilise les informations recueillies pour réduire les dimensions. Elle calcule des axes orthogonaux à partir des axes originaux des caractéristiques. Ce sont les axes des directions ayant une variance maximale. Ces vecteurs représentent les directions de variance maximale qui sont connues comme \"les composantes principales\". On crée ensuite les valeurs propres qui définissent la magnitude des composantes principales. **Les valeurs propres sont les composantes de l'ACP.**\n",
    "\n",
    "*Par conséquent, pour N dimensions, il y aura une matrice de variance-covariance NxN et par conséquent, nous aurons un vecteur propre de N valeurs et une matrice de N valeurs propres.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce que sont les vecteurs propres et les valeurs propres\n",
    "## Vecteur propre\n",
    "\n",
    "Si nous multiplions une matrice par un vecteur, nous obtenons un nouveau vecteur. La multiplication d'une matrice par un vecteur est connue sous le nom de matrices de transformation.\n",
    "\n",
    "Nous pouvons transformer et changer les matrices en nouveaux vecteurs en multipliant une matrice par un vecteur. La multiplication de la matrice par un vecteur permet de calculer un nouveau vecteur. C'est le vecteur transformé.\n",
    "\n",
    "- Le nouveau vecteur peut être considéré sous deux formes :\n",
    "    1. Parfois, le nouveau vecteur transformé est juste une forme mise à l'échelle du vecteur original. Cela signifie que le nouveau vecteur peut être recalculé en multipliant simplement un scalaire (nombre) au vecteur original.\n",
    "\n",
    "    2. D'autres fois, le vecteur transformé n'a pas de relation scalaire directe avec le vecteur original que nous avons utilisé pour le multiplier à la matrice.\n",
    "\n",
    "Si le nouveau vecteur transformé est juste une forme mise à l'échelle du vecteur original, alors le vecteur original est connu pour être un vecteur propre de la matrice originale. Les vecteurs qui présentent cette caractéristique sont des vecteurs spéciaux et sont appelés \"vecteurs propres\". Les vecteurs propres peuvent être utilisés pour représenter une matrice de grande dimension.\n",
    "![](img/vecteurspropres.gif)\n",
    "## Valeur propre\n",
    "Le scalaire qui est utilisé pour transformer (étirer) un vecteur propre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les composantes principales :\n",
    "\n",
    "Les composantes principales sont la clé de la PCA.  \n",
    "*Lorsque les données sont projetées dans une dimension inférieure (supposons trois dimensions) à partir d'un espace supérieur, les trois dimensions ne sont rien d'autre que les trois composantes principales qui capturent (ou conservent) la majeure partie de la variance (informations) de vos données.*\n",
    "\n",
    "- Les composantes principales ont à la fois une **direction et une amplitude**. \n",
    "    - La **direction** représente sur quels axes principaux les données sont principalement réparties ou ont le plus de variance.\n",
    "    - L'**amplitude** signifie la quantité de variance que la composante principale capture des données lorsqu'elles sont projetées sur cet axe. \n",
    "- Les composantes principales sont une ligne droite et la première composante principale contient le plus de variance des données. Chaque composante principale suivante est **orthogonale** à la dernière et a une variance moindre. \n",
    "De cette façon, étant donné un ensemble de x variables corrélées sur y échantillons, vous obtenez un ensemble de u composantes principales **non corrélées** sur les mêmes y échantillons.\n",
    "\n",
    "La raison pour laquelle vous obtenez des composantes principales non corrélés à partir des entités d'origine est que **les entitées corrélées contribuent à la même composante principale**.\n",
    "\n",
    "Chaque composante principale représente un ensemble différent de caractéristiques corrélées avec différentes quantités de variation.\n",
    "\n",
    "![](img/dependencies.png)\n",
    "\n",
    "Chaque composante principale représente un pourcentage de la variation totale capturée à partir des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisations :\n",
    "- Réduire le nombre de dimensions du dataset.\n",
    "- Trouver des patterns dans les datasets de grande dimension.\n",
    "- Pour visualiser les données de grande dimension.\n",
    "- Pour ignorer le bruit.\n",
    "- Améliorer la classification.\n",
    "- Obtenir une description compacte.\n",
    "- Capter autant que possible la variance originale des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications de la ACP :\n",
    "\n",
    "- Visualisation de données.\n",
    "- Compression de données.\n",
    "- Réduction du bruit.\n",
    "- Classification des données.\n",
    "- Compression d'images.\n",
    "- Reconnaissance des visages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres réductions de dimensions :\n",
    "\n",
    "- Il y a de nombreuses façons de faire de la réduction de dimensions, on peut citer :  \n",
    "    1. Analyse discriminante linéaire (LDA : Linear Discriminant Analysis):  \n",
    "    **utilisée pour compresser des données supervisées.**  \n",
    "    2. Analyse en composantes principales (PCA : Principal component analysis):  \n",
    "    **utilisée pour compresser des données non-supervisées**  \n",
    "    3. Analyse en composantes principales avec noyau (KDA : Kernel principal component analysis):  \n",
    "    **utilisée pour la réction de dimensions non linéaire.**  \n",
    "---\n",
    "## Implémentations en python : \n",
    "\n",
    "1. Linear Discriminant Analysis (LDA):  \n",
    "Lorsque nous disposons d'un grand ensemble de caractéristiques (features), que nos données sont normalement distribuées et que les caractéristiques ne sont pas corrélées entre elles, nous pouvons utiliser LDA pour réduire le nombre de dimensions.\n",
    "```python\n",
    "from sklearn.lda import LDA  \n",
    "my_lda = LDA(n_components=3)  \n",
    "lda_components = my_lda.fit_transform(X_train, Y_train)\n",
    "```  \n",
    "\n",
    "2. Principal component analysis (PCA):\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca_classifier = PCA(n_components=3)\n",
    "my_pca_components = pca_classifier.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "3. Kernel principal component analysis (KDA):\n",
    "\n",
    "Lorsque nous disposons de caractéristiques non linéaires, nous pouvons les projeter sur un ensemble de caractéristiques plus large afin de supprimer leurs corrélations et de les rendre linéaires.\n",
    "\n",
    "Essentiellement, les données non linéaires sont mises en correspondance et transformées dans un espace à plus haute dimension. L'ACP est ensuite utilisée pour réduire les dimensions. Cependant, l'un des inconvénients de cette approche est qu'elle est très coûteuse en termes de calcul.\n",
    "\n",
    "Comme dans l'ACP, nous calculons d'abord la matrice de variance-covariance, puis nous préparons les vecteurs propres et les valeurs propres ayant la variance la plus élevée pour calculer les composantes principales.\n",
    "\n",
    "Nous calculons ensuite la matrice des noyaux. Cela nous oblige à construire une matrice de similarité. La matrice est ensuite décomposée en créant des valeurs propres et des vecteurs propres.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=2,kernel='rbf', gamma=45)\n",
    "kpca_components = kpca.fit_transform(X)\n",
    "```\n",
    "*Gamma est un hyperparamètre du noyau rbf.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
